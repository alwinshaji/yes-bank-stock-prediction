{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - YesBank StockPrice Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Name**            - Alwin Shaji\n",
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This project focuses on predicting the **closing stock prices** of Yes Bank using machine learning techniques and historical stock data. The goal was to develop a reliable, accurate, and deployment-ready model that can assist investors, analysts, or financial platforms in forecasting future prices with minimal error and high confidence. The entire workflow covers data preprocessing, feature engineering, exploratory data analysis (EDA), model building, evaluation, optimization, and model deployment preparation.\n",
        "\n",
        "We began by importing the stock price dataset, which consisted of key variables like `Date`, `Open`, `High`, `Low`, and `Close`. The `Date` column was first cleaned and converted into a proper datetime format, allowing for temporal analysis. From this cleaned dataset, we engineered new features such as `Prev_Close` (previous day's close), `Daily_Change`, `Range`, and rolling averages to enrich the dataset and capture time-dependent behavior in price fluctuations. We also ensured that the dataset had no missing values or duplicate entries and sorted it chronologically to maintain data integrity.\n",
        "\n",
        "Following preprocessing, we performed exploratory data analysis (EDA) and created over **15 visualizations**, including line plots, bar charts, box plots, area charts, candlestick charts, and correlation heatmaps. These visualizations helped uncover patterns such as monthly volatility trends, the relationship between open vs. close prices, and periods of high fluctuation. The insights gained from EDA informed our modeling strategy by emphasizing the predictive strength of lag-based features.\n",
        "\n",
        "We implemented multiple regression models for prediction. **Linear Regression** was the first model and turned out to be the best-performing one. Trained using the `Prev_Close` feature, it produced an **RMSE of 3.08** and an **R² score of 0.9995**, indicating almost perfect prediction accuracy. This performance suggested a strong linear relationship between a day’s closing price and the previous day’s closing price in Yes Bank stock data.\n",
        "\n",
        "To experiment with more robust and non-linear models, we also implemented **Random Forest Regressor** and **Gradient Boosting Regressor (GBR)**. While both models were functional and captured more complex relationships, they initially underperformed compared to Linear Regression. We then optimized both models using **GridSearchCV**, tuning hyperparameters like `n_estimators`, `max_depth`, and `learning_rate`. The **tuned GBR** showed notable improvement, reducing RMSE from **1356.89 to 1228.87** and increasing R² from **0.7651 to 0.7873**.\n",
        "\n",
        "However, despite these improvements, **Linear Regression remained the most accurate model** and was chosen for final deployment. The model was saved using `joblib` into a `.joblib` file format and later reloaded successfully to test on unseen data. The prediction output confirmed that the model works reliably outside the training context, proving it is suitable for real-time or batch deployment scenarios.\n",
        "\n",
        "Evaluation metrics used across all models included **RMSE (Root Mean Squared Error)** to assess average prediction error in real price units, and **R² Score** to measure how well the model explains variance in the stock price. These metrics were visualized through comparison charts to highlight performance differences across models.\n",
        "\n",
        "In conclusion, this project demonstrated a complete machine learning pipeline, from raw data to deployment-ready model. It showcased skills in data wrangling, EDA, model building, evaluation, optimization, and final integration. The Linear Regression model, due to its simplicity and exceptional accuracy, serves as a dependable tool for Yes Bank stock price forecasting, contributing to data-driven decision-making in finance.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/alwinshaji/yes-bank-stock-prediction\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to predict the closing price of a stock based on its historical price data. Using past values like open, high, low prices and moving averages, we aim to build a regression model that can forecast future prices.\n",
        "\n",
        "This helps in understanding price trends, improving investment decisions, and showcasing the power of machine learning in financial forecasting."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Date handling\n",
        "from datetime import datetime\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 120\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data_YesBank_StockPrices.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows, cols = data.shape\n",
        "print(f\"Number of rows: {rows}\")\n",
        "print(f\"Number of columns: {cols}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_count = data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing = data.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising Missing Values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We checked for missing values in the dataset and found that there are **no missing values** in any column.\n",
        "\n",
        "Hence, no cleaning for nulls was required at this stage."
      ],
      "metadata": {
        "id": "P7IKFSJHVaY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains **historical stock prices of Yes Bank**, ranging from **July 2005 to November 2020**.\n",
        "\n",
        "✅ The dataset has:\n",
        "- **185 rows** and **5 columns**\n",
        "- **No missing values** or duplicate rows\n",
        "- A **clean and consistent time-series** structure with monthly data\n",
        "\n",
        "We used this data to perform exploratory analysis, engineer predictive features (like moving averages and volatility), and build machine learning models to **forecast future closing prices**.\n",
        "\n",
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset columns:\", data.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowing Your Variables\n",
        "\n",
        "The dataset contains the following 5 columns:\n",
        "\n",
        "1. **Date** – The trading date (monthly frequency)\n",
        "2. **Open** – Stock price at the beginning of the trading day\n",
        "3. **High** – Highest price the stock reached during the day\n",
        "4. **Low** – Lowest price during the day\n",
        "5. **Close** – Stock price at the end of the day (Target variable for prediction)\n",
        "\n",
        "All columns except `Date` are numeric and represent the price movement of Yes Bank stock over time. The `Close` column is the primary target for our regression model.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique values in each column\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert 'Date' to proper datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "# 2. Drop rows with invalid dates (if any)\n",
        "data.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "# 3. Sort data by date (chronological order)\n",
        "data = data.sort_values(by='Date')\n",
        "\n",
        "# 4. Reset index after sorting\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# 5. Final check for missing values or duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# 6. Create a backup just in case (optional)\n",
        "data_original = data.copy()\n",
        "\n",
        "# 7. Show data info to confirm\n",
        "data.info()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulations Performed\n",
        "\n",
        "To prepare the dataset for analysis, the following steps were taken:\n",
        "\n",
        "1. **Date Conversion**  \n",
        "   Converted the `Date` column from text format (`'Jul-05'`, `'Aug-06'`) to proper datetime using `pd.to_datetime()`.\n",
        "\n",
        "2. **Sorting Chronologically**  \n",
        "   Sorted the dataset by `Date` to ensure time-series consistency.\n",
        "\n",
        "3. **Handling Duplicates and Nulls**  \n",
        "   Checked for and removed any duplicate rows (there were none) and verified that the dataset contained **no missing values**.\n",
        "\n",
        "4. **Resetting Index**  \n",
        "   Reset the index after sorting to maintain clean row order.\n",
        "\n",
        "---\n",
        "\n",
        "### Early Insights from Data (Pre-EDA)\n",
        "\n",
        "- The dataset spans from **July 2005 to November 2020** with consistent monthly entries.\n",
        "- Price data (`Open`, `High`, `Low`, `Close`) appears clean and well-formatted.\n",
        "- There are **no missing values or anomalies** in the main numeric columns.\n",
        "- Stock prices show increasing and decreasing cycles — perfect for trend analysis.\n",
        "\n",
        "These preprocessing steps ensure the dataset is **clean, structured, and analysis-ready** for further EDA and modeling.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert Date to datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Chart 1 – Line plot of Close over time\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.lineplot(x='Date', y='Close', data=data)\n",
        "plt.title('Close Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line plot displays the trend of the stock’s closing price over time. It helps us visually understand the overall movement of the stock — whether it’s rising, falling, or showing volatility — and is a fundamental first step in any time-series analysis.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Close price shows visible trends, including sustained growth phases and periods of decline. Peaks and dips may correspond to financial events, market shifts, or external shocks.\n",
        "\n",
        "This pattern helps identify stable vs. volatile periods in the stock’s history.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the price trend allows investors and analysts to time their entries and exits more effectively. Recognizing long-term price movement helps inform strategy and enhances decision-making in portfolio management."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Line Chart for Open vs Close Prices\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Date'], data['Open'], label='Open Price', color='orange', linestyle='--')\n",
        "plt.plot(data['Date'], data['Close'], label='Close Price', color='blue')\n",
        "plt.title(\"Yes Bank - Open vs Close Prices Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dual-line chart was selected to compare the opening and closing prices of each month. This comparison helps analyze market behavior within the same period and shows how price sentiment changes by the end of the month.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights include identifying months where the stock closed higher or lower than it opened, indicating bullish or bearish sentiment. When such behavior is consistent, it suggests a trend in how the market treats the stock intra-month.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In predictive modeling, this chart enables the use of sentiment direction (gain/loss in a month) as a feature. This can improve the model’s ability to detect short-term momentum and better anticipate future closings.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Area Chart for Low prices over time\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.fill_between(data['Date'].values, data['Low'].values, color='skyblue', alpha=0.4)\n",
        "plt.plot(data['Date'], data['Low'], color='blue')\n",
        "plt.title(\"Yes Bank - Monthly Low Prices (Area Chart)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Low Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The area chart was used to visualize the monthly low prices in a way that emphasizes how deeply the stock has fallen during each period. The filled area makes it easier to see the extent of the price drop over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights include spotting recurring low-price levels that may act as support zones. This also helps identify periods of stress or bearish dominance where the stock consistently fell to lower thresholds.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights add value to the prediction model by defining risk levels and historical minimums, aiding in predicting lower bounds or volatility. It helps traders and investors set protective measures like stop-loss levels.\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "width = 3  # Width of the candle body\n",
        "\n",
        "for i in range(len(data)):\n",
        "    date = data['Date'].iloc[i]\n",
        "    open_price = data['Open'].iloc[i]\n",
        "    close_price = data['Close'].iloc[i]\n",
        "    high = data['High'].iloc[i]\n",
        "    low = data['Low'].iloc[i]\n",
        "\n",
        "    color = 'green' if close_price >= open_price else 'red'\n",
        "    lower = min(open_price, close_price)\n",
        "    height = abs(close_price - open_price)\n",
        "\n",
        "    # Wick (Low to High) with sentiment color\n",
        "    plt.vlines(date, low, high, color=color, linewidth=1)\n",
        "\n",
        "    # Candle body (Open to Close)\n",
        "    plt.bar(date, height, bottom=lower, width=width, color=color, edgecolor='black')\n",
        "\n",
        "# Legend for color coding\n",
        "green_patch = mpatches.Patch(color='green', label='Bullish (Close > Open)')\n",
        "red_patch = mpatches.Patch(color='red', label='Bearish (Open > Close)')\n",
        "plt.legend(handles=[green_patch, red_patch], loc='upper right')\n",
        "\n",
        "plt.title(\"Yes Bank - Simulated Candlestick Chart with Sentiment Legend\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A candlestick-style chart was used to visually integrate high, low, open, and close prices. It gives a compact yet comprehensive view of monthly price movements and their structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights involve recognizing price behaviors such as whether the month was bullish (close > open) or bearish (open > close), and how large the price swings were within that period. Patterns similar to technical candlesticks can be detected.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "positively impacts predictions by allowing the model to learn from price behavior types and structure. Incorporating candlestick-based signals can improve timing and classification of potential price movements"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate 6-Month Rolling Average of Close Prices\n",
        "data['Close_MA_6'] = data['Close'].rolling(window=6).mean()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data['Date'], data['Close'], label='Actual Close', color='darkgray')\n",
        "plt.plot(data['Date'], data['Close_MA_6'], label='6-Month Moving Average', color='blue', linewidth=2)\n",
        "plt.title(\"Yes Bank - Close Price with 6-Month Moving Average\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This moving average chart was used to smooth out fluctuations in closing prices and focus on longer-term trends. It's a widely used method in time series to reduce noise and detect consistent patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key insight is how the general direction of the stock (uptrend or downtrend) is evolving. It helps distinguish between short-term spikes and sustained growth or decline phases.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By integrating this smoothed trend into the model, we reduce the risk of overfitting to temporary noise and improve long-range forecasting. This supports more stable, reliable price predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Volatility as the difference between High and Low\n",
        "data['Volatility'] = data['High'] - data['Low']\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data['Date'], data['Volatility'], color='purple')\n",
        "plt.fill_between(data['Date'].values, data['Volatility'].values, color='purple', alpha=0.3)\n",
        "plt.title(\"Yes Bank - Monthly Price Volatility (High - Low)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price Range\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was used to measure monthly volatility by calculating the difference between the high and low prices. It gives a direct view of how much the stock swings within each month.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights include identifying periods of high price fluctuation which may reflect uncertainty, news events, or trading volume shifts. Low volatility months suggest stability, while high ones hint at risk or opportunity.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporating volatility into the model helps in forecasting not just the price but also the confidence interval of the prediction. It informs how cautious or aggressive strategies should be, depending on market behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Monthly returns as percentage\n",
        "data['Monthly_Return_%'] = data['Close'].pct_change() * 100\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data['Date'], data['Monthly_Return_%'], color='teal', marker='o')\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "plt.title(\"Yes Bank - Monthly Percentage Returns\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Return (%)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows how much the stock price changes each month in percentage. It helps us see gains and losses more clearly than raw prices. The purpose is to understand short-term trends and volatility."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals patterns like stable growth, frequent drops, or sudden spikes. These patterns help spot when the stock was consistent or unpredictable."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Such insights help build better forecasts. Knowing when returns were steady or volatile supports smarter timing and risk control in trading strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate spread\n",
        "data['High_Low_Spread'] = data['High'] - data['Low']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data['Date'], data['High_Low_Spread'], color='darkorange')\n",
        "plt.title(\"Yes Bank - High vs. Low Price Spread Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Spread\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the difference between the highest and lowest prices in each month. It helps to identify how much the stock fluctuated during that period."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger spreads indicate more volatility or uncertainty in the market. Smaller spreads show more stable and predictable price behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By spotting volatile months, investors can avoid risky entry points. This helps improve timing and build a more stable trend-based strategy.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature formula: Range = High - Low\n",
        "data['Range'] = data['High'] - data['Low']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x='Range', y='Close', data=data)\n",
        "plt.title(\"Scatter Plot – Range (High - Low) vs Close Price\")\n",
        "plt.xlabel(\"Daily Price Range\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This chart helps visualize how daily price volatility (range) may relate to the stock's closing value. It's used to detect whether larger intraday swings lead to higher or lower closing prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows that the Close price occurs across all range sizes, meaning volatility doesn’t directly influence where the day ends. Most ranges are clustered in a tight zone, indicating lower daily fluctuations are more common."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traders can’t rely on range alone to predict closing price but may use it to assess risk. Combining range with other signals may help create safer, volatility-aware trade setups.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature formula: Price_Change = Close - Open\n",
        "data['Price_Change'] = data['Close'] - data['Open']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x='Price_Change', y='Close', data=data)\n",
        "plt.title(\"Scatter Plot – Price Change vs Close Price\")\n",
        "plt.xlabel(\"Price Change (Close - Open)\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price Change is a key indicator of daily momentum, and this chart shows how strongly intraday movements correspond to final prices. It’s especially helpful for short-term traders.\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The chart reveals that large positive or negative changes are rare and don’t necessarily align with extreme Close prices. Close price stays centered across various price change values.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature highlights market strength or weakness within a day. Businesses can use this to fine-tune trade timing, especially for intraday strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MA10 = 10-day moving average of Close\n",
        "# MA20 = 20-day moving average of Close\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
        "data['MA20'] = data['Close'].rolling(window=20).mean()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data['Date'], data['Close'], label='Close Price')\n",
        "plt.plot(data['Date'], data['MA10'], label='MA10')\n",
        "plt.plot(data['Date'], data['MA20'], label='MA20')\n",
        "plt.title(\"Line Plot – Moving Averages vs Close Price\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving averages smooth price data and help reveal the market trend direction. Comparing them to the Close price tells us how reactive the averages are.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " MA10 tracks Close more closely than MA20, showing it reacts faster to price swings. The crossover points between MA10 and MA20 often align with trend changes.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These indicators support momentum-based strategies and automated trade signals. Investors can use them to time entries or exits more reliably."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature formula: Volatility = 10-day rolling standard deviation of Close\n",
        "data['Volatility'] = data['Close'].rolling(window=10).std()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x='Date', y='Volatility', data=data)\n",
        "plt.title(\"Line Plot – Volatility Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Volatility\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volatility shows how unpredictable the market is. This chart was chosen to observe how risk varies over time."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volatility spikes are seen in specific periods, likely due to news or market shocks. Calm periods show tight ranges and low risk."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps manage risk — high volatility days can trigger stop-loss adjustments or smaller position sizing. It's a critical input for risk-focused models."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature formula: Return = (Close_t - Close_t-1) / Close_t-1\n",
        "data['Prev_Close'] = data['Close'].shift(1)\n",
        "data['Return'] = (data['Close'] - data['Prev_Close']) / data['Prev_Close']\n",
        "data.dropna(inplace=True)  # remove NaNs from shifting\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data['Return'], bins=50, kde=True)\n",
        "plt.title(\"Histogram – Daily Returns\")\n",
        "plt.xlabel(\"Daily Return\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart reveals how often gains or losses of various sizes occur. It helps analyze profitability and risk."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The return distribution is centered near zero but has outliers in both directions. Most trading days result in small changes, but large spikes do happen."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Understanding return patterns helps in setting profit targets, stop-loss levels, and modeling financial risk. It's also vital for strategies using Sharpe ratio."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define basic features\n",
        "basic_features = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(data[basic_features].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap of Basic Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Correlation heatmaps help to quickly identify relationships between numeric variables. This is particularly important in financial data where features like Open, High, Low, and Close often move together.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, strong positive correlations were found between all price points, especially between `High` and `Close`, and `Open` and `Close`. This indicates that these features rise and fall together."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data[basic_features])\n",
        "plt.suptitle(\"Pair Plot of Basic Features\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A pair plot helps visualize both the distribution and pairwise relationships between numerical features in one grid. It's ideal for identifying trends, linear relationships, or outliers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most feature pairs (like Open vs Close or High vs Low) show a strong linear relationship. The distribution plots on the diagonal reveal that most price values are centered around specific ranges with mild skewness.\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Hypothesis 1: Trend & Risk Analysis\n",
        "\"Periods of high volatility tend to follow large price movements, suggesting that strong rallies or dips increase uncertainty and risk.\n",
        "Using the volatility and return charts, we hypothesize that investors should scale down position sizes or hedge after unusually strong up or down days to protect capital.\"\n",
        "\n",
        "✅ Hypothesis 2: Momentum-Based Trading\n",
        "\"Short-term moving averages (MA10) crossing above long-term averages (MA20) often precede upward price trends.\n",
        "From the moving average and candlestick charts, we hypothesize that these crossover points can be used to generate entry signals in a momentum-based trading strategy.\"\n",
        "\n",
        "✅ Hypothesis 3: Support Level & Price Recovery\n",
        "\"Sustained dips to historical low-price zones (shown in the area chart) often precede a rebound within the next 2–4 months.\n",
        "We hypothesize that tracking area chart patterns in combination with 6-month rolling averages can help identify oversold zones where value investors can initiate long positions.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Research Hypothesis:\n",
        "\"Large price movements are followed by increased volatility.\n",
        "\n",
        "✅ Null Hypothesis (H₀):\n",
        "There is no significant difference in volatility following large price movements compared to normal price movements.\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):\n",
        "Volatility is significantly higher following large price movements compared to normal price movements.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Fix Date column format: 'Jul-05', 'Aug-05', etc.\n",
        "data['Date'] = pd.to_datetime(data['Date'], format='%b-%y')\n",
        "\n",
        "# Feature Engineering\n",
        "data['Return'] = data['Close'].pct_change()\n",
        "data['Volatility'] = data['Close'].rolling(window=10).std()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Top 10% return threshold\n",
        "threshold = data['Return'].quantile(0.90)\n",
        "\n",
        "# Group volatility by return size\n",
        "high_movement_days = data[data['Return'] >= threshold]['Volatility']\n",
        "normal_days = data[data['Return'] < threshold]['Volatility']\n",
        "\n",
        "# Welch’s T-test\n",
        "t_stat, p_value = ttest_ind(high_movement_days, normal_days, equal_var=False)\n",
        "\n",
        "p_value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Welch’s t-test, a statistical method to compare the average volatility between days with high price movements and normal days. This test is ideal when the two groups may have unequal variances. The resulting p-value tells us whether the difference in volatility is statistically significant."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Welch’s t-test because it’s designed for comparing the means of two independent groups when their variances may differ, which is common in financial data. It’s more reliable than the regular t-test when dealing with uneven volatility across different market conditions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "✅ Research Statement: You want to know whether MA10 crossing above MA20 (a bullish signal) is followed by a statistically higher average price than normal.\n",
        "\n",
        "✅ Null Hypothesis (H₀):There is no significant difference in average closing price after MA10 crosses above MA20 compared to other times.\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):The average closing price is significantly higher after MA10 crosses above MA20 compared to other times.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Moving Averages\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
        "data['MA20'] = data['Close'].rolling(window=20).mean()\n",
        "\n",
        "# Signal where MA10 crosses above MA20\n",
        "data['Signal'] = (data['MA10'] > data['MA20']) & (data['MA10'].shift(1) <= data['MA20'].shift(1))\n",
        "\n",
        "# Split into two groups\n",
        "bullish_days = data[data['Signal']]['Close'].dropna()\n",
        "non_bullish_days = data[~data['Signal']]['Close'].dropna()\n",
        "\n",
        "# Mann–Whitney U Test (two-sided)\n",
        "stat, p_value = mannwhitneyu(bullish_days, non_bullish_days, alternative='greater')\n",
        "\n",
        "p_value\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Mann–Whitney U test, a non-parametric alternative to the t-test, to compare the closing prices after moving average crossovers with normal days."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test was chosen because it doesn't assume a normal distribution and is ideal for detecting whether one group tends to have higher values than the other — perfect for financial data with outliers or skewed distributions."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Research Statement: \"Stock prices tend to recover after reaching historically low levels.\"\n",
        "\n",
        "✅ Null Hypothesis (H₀):The distribution of future returns after hitting low price zones is the same as that of all other times.\n",
        "\n",
        "✅ Alternate Hypothesis (H₁):The distribution of future returns after hitting low price zones is significantly higher (i.e., shows recovery).\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Create return feature\n",
        "data['Return'] = data['Close'].pct_change()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define \"low price zone\" as bottom 10% Close prices\n",
        "low_price_threshold = data['Close'].quantile(0.10)\n",
        "\n",
        "# Get future returns after low prices vs regular returns\n",
        "low_days = data[data['Close'] <= low_price_threshold]['Return']\n",
        "normal_days = data[data['Close'] > low_price_threshold]['Return']\n",
        "\n",
        "# Perform Kolmogorov–Smirnov test\n",
        "ks_stat, p_value = ks_2samp(low_days, normal_days, alternative='less')  # Test if low_days < normal_days\n",
        "\n",
        "p_value\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used the Kolmogorov–Smirnov test to compare the distribution of future returns after low-price events with regular periods."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K–S test compares entire distributions rather than just means (like a t-test) or ranks (like Mann–Whitney).\n",
        "It tells us if the pattern of returns is statistically different after low-price events — including shifts in volatility, shape, and central tendency.\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Display if any column has missing values\n",
        "print(\"Missing values per column:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After checking for missing values in the dataset, we found that there were **no null or missing entries** in any column.\n",
        "\n",
        "As a result, no imputation was needed."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for 'Close' price to check outliers\n",
        "sns.boxplot(x=data['Close'])\n",
        "plt.title(\"Boxplot of Close Price\")\n",
        "plt.show()\n",
        "\n",
        "# IQR method\n",
        "Q1 = data['Close'].quantile(0.25)\n",
        "Q3 = data['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Detect\n",
        "outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]\n",
        "print(f\"Number of outliers in Close price: {len(outliers)}\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Treatment\n",
        "\n",
        "We first visualized the `Close` price distribution using a boxplot and detected outliers using the **IQR (Interquartile Range)** method. This approach defines outliers as data points that fall outside 1.5×IQR below the 25th percentile or above the 75th percentile.\n",
        "\n",
        "#### However, no outlier removal or transformation was applied. Here's why:\n",
        "\n",
        "- The dataset represents **real stock market behavior**, where price spikes and drops are often caused by actual market events.\n",
        "- In time-series financial data, what may appear as an outlier statistically could reflect **genuine, meaningful volatility** (e.g., news events, earnings, macro factors).\n",
        "- Removing these values may result in **loss of signal**, especially for a predictive model.\n",
        "\n",
        "#### Technique Considered:\n",
        "- **IQR Method**: Used for detection only — no rows were removed.\n",
        "\n",
        "This choice ensures that the model learns from all market patterns, including sharp rises and falls in price.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We checked the dataset for categorical columns.\n",
        "\n",
        "✅ In this case, no categorical columns were found, as the dataset consists only of numerical and datetime variables.\n",
        "\n",
        "Had there been any, we would have applied **one-hot encoding** to convert them into a suitable format for machine learning models."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Summary**"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although this project does not involve any textual columns, it is important to understand how text data is typically prepared for machine learning.\n",
        "\n",
        "Textual data is unstructured by nature and must be cleaned and transformed into a structured numerical format before it can be used by models.\n",
        "\n",
        "#### 🔧 Common Text Preprocessing Steps:\n",
        "\n",
        "1. **Lowercasing**  \n",
        "   Converts all text to lowercase to avoid case-based mismatches (e.g., \"Bank\" vs \"bank\").\n",
        "\n",
        "2. **Removing Punctuation and Special Characters**  \n",
        "   Strips out unnecessary symbols, digits, or formatting artifacts.\n",
        "\n",
        "3. **Removing Stopwords**  \n",
        "   Eliminates common, less meaningful words (like \"the\", \"is\", \"in\") using predefined stopword lists.\n",
        "\n",
        "4. **Tokenization**  \n",
        "   Splits sentences or documents into individual words or tokens.\n",
        "\n",
        "➡️ Since the current dataset contains only structured numerical data, no textual preprocessing was required.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new features based on price behavior\n",
        "data['Range'] = data['High'] - data['Low']                          # Price volatility\n",
        "data['Price_Change'] = data['Close'] - data['Open']                # Daily momentum\n",
        "data['Prev_Close'] = data['Close'].shift(1)                        # Previous day's close\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()             # 10-day moving average\n",
        "data['MA20'] = data['Close'].rolling(window=20).mean()             # 20-day moving average\n",
        "data['Return'] = data['Close'].pct_change()                        # Daily % return\n",
        "data['Volatility'] = data['Close'].rolling(window=10).std()        # Rolling std deviation (volatility)\n",
        "\n",
        "# Drop rows with NaNs from rolling/shift operations\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check correlation among numerical features\n",
        "engineered_cols = ['Prev_Close', 'MA10', 'MA20', 'Range', 'Price_Change', 'Return', 'Volatility']\n",
        "corr_matrix = data[engineered_cols].corr()\n",
        "\n",
        "# Plot heatmap to visualize correlation\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix of Engineered Features\")\n",
        "plt.show()\n",
        "\n",
        "# Manual selection based on low multicollinearity and predictive logic\n",
        "# You can adjust based on heatmap values\n",
        "selected_features = ['Prev_Close', 'MA10', 'Range', 'Price_Change', 'Volatility', 'Return']\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure that only the most relevant and non-redundant features are passed to the model, we applied a combination of **correlation-based filtering** and **domain knowledge-based selection**.\n",
        "\n",
        "#### 1. Correlation Matrix (Filter Method)\n",
        "We calculated the correlation matrix between all engineered features and visualized it using a heatmap. This helped us:\n",
        "\n",
        "- Identify **highly correlated pairs** (e.g., MA10 and MA20)\n",
        "- Avoid multicollinearity, which can distort model interpretation\n",
        "- Select a minimal set of features that are **individually informative** and **not redundant**\n",
        "\n",
        "➡️ Based on this, we excluded one of the highly correlated moving averages (`MA20`), keeping `MA10` as it was more responsive to short-term trends.\n",
        "\n",
        "#### 2. Business/Domain Logic\n",
        "We selected features that have **clear financial meaning** and are commonly used in stock analysis.\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Features Identified and Why\n",
        "\n",
        "After performing feature engineering and examining the correlation matrix, the following features were selected as most important for predicting the stock's closing price:\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 1. **Prev_Close**\n",
        "- Represents the previous day's closing price.\n",
        "- Stock prices typically follow momentum — the previous close is a strong predictor for the next day's movement.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 2. **MA10 (10-Day Moving Average)**\n",
        "- Captures short-term trend behavior.\n",
        "- Helps smooth out noise and highlights trend direction over the recent period.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 3. **Range (High - Low)**\n",
        "- Measures daily price volatility.\n",
        "- A high range often signals uncertainty or momentum — both can affect the closing price.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 4. **Price_Change (Close - Open)**\n",
        "- Indicates intraday momentum.\n",
        "- If the stock closed much higher/lower than it opened, it reflects buying or selling pressure during the day.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 5. **Volatility (10-day Std Dev of Close)**\n",
        "- Represents risk or fluctuation in recent prices.\n",
        "- Higher volatility can affect how much the stock is likely to move on a given day.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📌 6. **Return (% change of Close)**\n",
        "- A normalized version of price movement.\n",
        "- Helps capture proportional gains/losses regardless of price scale.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Why These Were Selected:\n",
        "- All features have **clear financial intuition**.\n",
        "- They are **not highly correlated with each other**, which helps the model avoid redundancy.\n",
        "- Each one contributes a unique signal related to trend, momentum, or volatility — all crucial in stock price forecasting.\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the dataset suitable for regression modeling and trend analysis, the following data transformations were applied:\n",
        "\n",
        "1. **Datetime Conversion**  \n",
        "   Converted the `Date` column from string format (`Jul-05`, `Aug-05`) into proper `datetime` objects using `pd.to_datetime()`, which enabled time-based sorting and visualization.\n",
        "\n",
        "2. **Sorting by Date**  \n",
        "   Sorted the dataset chronologically to preserve the natural time-series order for modeling and trend discovery.\n",
        "\n",
        "3. **Lag and Rolling Features**  \n",
        "   Transformed raw price data by creating lag features (`Prev_Close`) and rolling features (`MA10`, `MA20`, `Volatility`) to capture past trends and momentum.\n",
        "\n",
        "4. **Mathematical Derivations**  \n",
        "   Applied basic arithmetic transformations to derive:\n",
        "   - `Range` = High - Low\n",
        "   - `Price_Change` = Close - Open\n",
        "   - `Return` = Daily percent return from previous close\n",
        "\n",
        "5. **Missing/Redundant Data Handling**  \n",
        "   Dropped rows with NaN values generated from lag/rolling operations and removed any unnecessary columns.\n",
        "\n",
        "➡️ These transformations helped expose **underlying trends**, **volatility patterns**, and **momentum indicators**, which are essential for improving model accuracy in stock price forecasting.\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Prev_Close', 'MA10', 'Range', 'Price_Change', 'Volatility', 'Return']\n",
        "target = 'Close'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform features\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the **StandardScaler** method to scale our features.\n",
        "\n",
        "📌 **What does it do?**  \n",
        "StandardScaler transforms the data so that each feature has:\n",
        "- A **mean of 0**\n",
        "- A **standard deviation of 1**\n",
        "\n",
        "📌 **Why did we use it?**\n",
        "Because one of our models, **Linear Regression**, is sensitive to the scale of features.  \n",
        "For example:\n",
        "- If one feature is in rupees (0–1000) and another is in decimals (0–1), Linear Regression might give unfair importance to the bigger number.\n",
        "- StandardScaler helps make all features equal in weight by **putting them on the same scale**.\n",
        "\n",
        "📌 **When is it needed?**\n",
        "- Needed for models like **Linear Regression, Logistic Regression, KNN, SVM**\n",
        "- Not needed for models like **Random Forest, Decision Trees**\n",
        "\n",
        "➡️ In our case, we used **StandardScaler only for Linear Regression**.  \n",
        "For Random Forest, we used the **original (unscaled) data**, since tree-based models don’t care about feature scale.\n"
      ],
      "metadata": {
        "id": "fKhxjpADqZUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In our case, we are working with a **small number of meaningful features** (6–7), all of which are:\n",
        "- Intuitive and easy to interpret\n",
        "- Carefully selected to avoid redundancy\n",
        "\n",
        "➡️ Therefore, dimensionality reduction was **not required or applied** for this project."
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your features and target\n",
        "features = ['Prev_Close', 'MA10', 'Range', 'Price_Change', 'Volatility', 'Return']\n",
        "target = 'Close'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Use 80% for training, 20% for testing — without shuffling\n",
        "train_size = int(0.8 * len(data))\n",
        "\n",
        "X_train = X.iloc[:train_size]\n",
        "X_test = X.iloc[train_size:]\n",
        "\n",
        "y_train = y.iloc[:train_size]\n",
        "y_test = y.iloc[train_size:]\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We split the dataset using an **80:20 ratio**:\n",
        "\n",
        "- **80% for training** the model\n",
        "- **20% for testing** its performance on unseen data\n",
        "\n",
        "#### 📌 Why 80:20?\n",
        "\n",
        "- It’s a **standard and balanced choice** — provides enough data for the model to learn while keeping enough unseen data for a **realistic evaluation**.\n",
        "- In time-series data, it’s especially important to preserve order, so we used the **first 80% for training** and the **last 20% for testing**, simulating a **real-world forecasting scenario**.\n",
        "\n",
        "➡️ This approach avoids data leakage, respects time order, and gives us a clear view of how well the model would perform on future stock prices.\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the data is not imbalanced because this is a **regression problem**, not a classification task.\n",
        "\n",
        "We are predicting a continuous numeric value — the **stock's closing price** — rather than class labels (like \"fraud\" vs \"not fraud\"). Imbalance only applies when target classes are heavily skewed, which is not the case here.\n",
        "\n",
        "Therefore, **no balancing techniques** are required for this dataset.\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Initialize and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 3. Evaluate the model\n",
        "rmse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"✅ Model trained successfully!\")\n",
        "print(f\"📉 RMSE: {rmse:.2f}\")\n",
        "print(f\"📊 R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 🔹 What is Linear Regression?\n",
        "Linear Regression is a supervised learning algorithm used to predict a continuous value by finding the best-fitting straight line between input features and the target.\n",
        "\n",
        "## 📊 Model Evaluation Metrics\n",
        "\n",
        "| Metric   | Description                                   | Ideal Value     |\n",
        "|----------|-----------------------------------------------|-----------------|\n",
        "| **RMSE** | Average prediction error (lower is better)    | As low as possible |\n",
        "| **R²**   | How well the model explains the target's variance | Closer to 1 is better |\n",
        "\n",
        "- **RMSE** gives the average distance between the predicted and actual values.\n",
        "- **R² Score** tells us how much of the variation in the target is explained by the model.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Example Results\n",
        "\n",
        "- 📉 **RMSE**: `5.43` → On average, predictions are off by ~5.43 units.\n",
        "- 📊 **R² Score**: `0.8923` → The model explains **89.23%** of the variance in the test set.\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use your actual model values\n",
        "metrics = ['RMSE', 'R² Score']\n",
        "values = [4.51, 0.9997]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen'])\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0, max(values) + 1)\n",
        "\n",
        "# Annotate bar values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, f\"{yval:.4f}\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create lag feature\n",
        "data['Prev_Close'] = data['Close'].shift(1)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Features and target\n",
        "X = data[['Prev_Close']]\n",
        "y = data['Close']\n",
        "\n",
        "# Train-test split without shuffle to preserve time sequence\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Align test dates (last n rows of data['Date'])\n",
        "test_dates = data['Date'].iloc[-len(y_test):].reset_index(drop=True)\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(test_dates, y_test.values, label='Actual', color='blue')\n",
        "plt.plot(test_dates, y_pred, label='Predicted', color='red')\n",
        "plt.title(\"Actual vs Predicted Close Price (Test Set)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "\n",
        "# Format x-axis as years\n",
        "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zcg6OKOI1SGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
        "\n",
        "# Initialize Ridge model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Grid search with 5-fold CV\n",
        "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and parameters\n",
        "best_ridge = grid_search.best_estimator_\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Predict using the best model\n",
        "y_pred = best_ridge.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "rmse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"✅ Ridge Regression Model trained with Grid Search!\")\n",
        "print(f\"🔧 Best Alpha: {best_alpha}\")\n",
        "print(f\"📉 RMSE: {rmse:.2f}\")\n",
        "print(f\"📊 R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Grid Search Cross-Validation to optimize the alpha hyperparameter in Ridge Regression.\n",
        "It systematically searches across a defined set of values to find the one that minimizes error on cross-validated folds.\n",
        "\n",
        "Grid Search was chosen because it’s a simple yet powerful method that guarantees an exhaustive search within the given range.\n",
        "It's especially effective when the hyperparameter space is small and the goal is to ensure optimal performance without guesswork.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There was no improvement after applying Ridge Regression with hyperparameter tuning.\n",
        "In fact, the RMSE increased drastically and the R² score dropped, indicating that the regularization negatively affected the model's ability to fit the data — most likely due to over-penalizing weights."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model names and scores\n",
        "models = ['Linear Regression', 'Ridge Regression (α=100)']\n",
        "rmse_scores = [3.08, 889.93]\n",
        "r2_scores = [0.9995, 0.8459]\n",
        "\n",
        "# RMSE Bar Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(models, rmse_scores, color=['skyblue', 'salmon'])\n",
        "plt.title('RMSE Comparison')\n",
        "plt.ylabel('RMSE (Lower is Better)')\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "# R² Score Bar Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, r2_scores, color=['skyblue', 'salmon'])\n",
        "plt.title('R² Score Comparison')\n",
        "plt.ylabel('R² (Higher is Better)')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1cT_syc9uYLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method that builds multiple decision trees and averages their outputs for better accuracy and robustness.\n",
        "It captures complex, non-linear relationships and reduces overfitting, making it suitable for real-world financial datasets with more variability or noise.\n",
        "\n",
        " Evaluation Metrics Used for Random Forest:\n",
        "\n",
        "✅RMSE (Root Mean Squared Error)\n",
        "\n",
        "Measures the average error between predicted and actual stock prices.\n",
        "\n",
        "Lower RMSE indicates better prediction accuracy.\n",
        "\n",
        "✅R² Score (Coefficient of Determination)\n",
        "\n",
        "Indicates how well the model explains the variance in the actual values.\n",
        "\n",
        "A score closer to 1 means the model is highly accurate in capturing patterns in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ5_1eYG0mCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"✅ Random Forest Regressor Model Trained\")\n",
        "print(f\"📉 RMSE: {rmse_rf:.2f}\")\n",
        "print(f\"📊 R² Score: {r2_rf:.4f}\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Random Forest metrics\n",
        "rf_model_name = ['Random Forest']\n",
        "rf_rmse = [1397.80]\n",
        "rf_r2 = [0.7580]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# 📉 RMSE Bar Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(rf_model_name, rf_rmse, color='lightgreen')\n",
        "plt.title('Random Forest - RMSE')\n",
        "plt.ylabel('RMSE (Lower is Better)')\n",
        "plt.ylim(0, max(rf_rmse)*1.2)\n",
        "\n",
        "# 📊 R² Score Bar Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(rf_model_name, rf_r2, color='lightgreen')\n",
        "plt.title('Random Forest - R² Score')\n",
        "plt.ylabel('R² (Higher is Better)')\n",
        "plt.ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Extract matching test dates\n",
        "test_dates_rf = data['Date'].iloc[-len(y_test):].reset_index(drop=True)\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(test_dates_rf, y_test.values, label='Actual', color='blue')\n",
        "plt.plot(test_dates_rf, y_pred_rf, label='Predicted (RF)', color='green')\n",
        "plt.title(\"Random Forest: Actual vs Predicted Close Price\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "\n",
        "# Format x-axis as years\n",
        "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Hyperparameter Optimization using GridSearchCV\n",
        "# -------------------------------\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 2. Fit the Algorithm\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"✅ Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 3. Predict on the model\n",
        "\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate\n",
        "\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"📉 RMSE (RF with GridSearch): {rmse_rf:.2f}\")\n",
        "print(f\"📊 R² Score: {r2_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV as our hyperparameter optimization technique for the Random Forest Regressor.\n",
        "It was chosen because it exhaustively tests different combinations of parameters and ensures the most optimal model configuration based on cross-validated performance, making it ideal when the parameter space is small and well-defined.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was a slight improvement in both RMSE and R² after applying GridSearchCV.\n",
        "Although the performance gain is modest, the optimized model fits the data slightly better and reduces prediction error, showing that even minimal tuning can help refine ensemble models like Random Forest.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model names and scores\n",
        "models = ['Random Forest (Default)', 'Random Forest (GridSearchCV)']\n",
        "rmse_scores = [1397.80, 1366.10]\n",
        "r2_scores = [0.7580, 0.7635]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# 📉 RMSE Bar Chart\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(models, rmse_scores, color=['lightgreen', 'orange'])\n",
        "plt.title('RMSE Comparison')\n",
        "plt.ylabel('RMSE (Lower is Better)')\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "# 📊 R² Bar Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, r2_scores, color=['lightgreen', 'orange'])\n",
        "plt.title('R² Score Comparison')\n",
        "plt.ylabel('R² (Higher is Better)')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "scGFRb1SzF7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact of the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE measures the average prediction error in price, helping assess how close model forecasts are to actual values.\n",
        "\n",
        "R² Score indicates how much of the variation in closing prices is explained by the model, showing its overall accuracy.\n",
        "\n",
        "The Random Forest model supports the business by providing reliable price forecasts that aid in better decision-making for trading and risk management."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ML Model - 3 Implementation\n",
        "\n",
        "gbr_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "y_pred_gbr = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "\n",
        "rmse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "print(\"✅ Gradient Boosting Regressor Model Trained\")\n",
        "print(f\"📉 RMSE: {rmse_gbr:.2f}\")\n",
        "print(f\"📊 R² Score: {r2_gbr:.4f}\")\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting is a powerful ensemble method that builds trees sequentially, with each tree correcting the errors of the previous one. It’s known for delivering strong predictive performance, especially when tuned well, and it can handle non-linear relationships better than basic models.\n",
        "\n",
        "📊 Evaluation Metrics to Use:\n",
        "\n",
        "RMSE – To measure average prediction error in real price units.\n",
        "\n",
        "R² Score – To assess how much variance in price the model explains."
      ],
      "metadata": {
        "id": "gLKQ6T-L2Vs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GBR Metrics\n",
        "gbr_model_name = ['Gradient Boosting']\n",
        "gbr_rmse = [1356.89]\n",
        "gbr_r2 = [0.7651]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# RMSE Chart\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(gbr_model_name, gbr_rmse, color='gold')\n",
        "plt.title('Gradient Boosting - RMSE')\n",
        "plt.ylabel('RMSE (Lower is Better)')\n",
        "plt.ylim(0, max(gbr_rmse)*1.2)\n",
        "\n",
        "# R² Score Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(gbr_model_name, gbr_r2, color='gold')\n",
        "plt.title('Gradient Boosting - R² Score')\n",
        "plt.ylabel('R² (Higher is Better)')\n",
        "plt.ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Define Parameter Grid for Tuning\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# 2. Initialize Base Model\n",
        "\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# 3. Apply GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=gbr,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. Fit the Algorithm\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_gbr_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"✅ Best Parameters Found: {grid_search.best_params_}\")\n",
        "\n",
        "# 5. Predict on the Model\n",
        "\n",
        "y_pred_gbr = best_gbr_model.predict(X_test)\n",
        "\n",
        "\n",
        "# 6. Evaluate Performance\n",
        "\n",
        "rmse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "print(f\"📉 RMSE (Tuned GBR): {rmse_gbr:.2f}\")\n",
        "print(f\"📊 R² Score (Tuned GBR): {r2_gbr:.4f}\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV as the hyperparameter optimization technique for Gradient Boosting Regressor.\n",
        "It was chosen because it exhaustively tests all combinations of specified hyperparameters to ensure we select the best-performing model configuration based on cross-validated error, which is ideal when the search space is manageable and accuracy is critical."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a clear improvement after applying hyperparameter tuning to the Gradient Boosting Regressor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model names and metrics\n",
        "models = ['Gradient Boosting (Default)', 'Gradient Boosting (GridSearchCV)']\n",
        "rmse_scores = [1356.89, 1228.87]\n",
        "r2_scores = [0.7651, 0.7873]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# RMSE Chart\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(models, rmse_scores, color=['gold', 'green'])\n",
        "plt.title('RMSE Comparison')\n",
        "plt.ylabel('RMSE (Lower is Better)')\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "# R² Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, r2_scores, color=['gold', 'green'])\n",
        "plt.title('R² Score Comparison')\n",
        "plt.ylabel('R² (Higher is Better)')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pdDgvjpl7B8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered RMSE and R² Score to evaluate the model’s effectiveness for positive business impact.\n",
        "RMSE helps measure how close our predictions are to actual stock prices (minimizing financial error), while R² shows how well the model explains market behavior — both are essential for making confident, data-driven investment or trading decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Linear Regression (without tuning) as the final prediction model.\n",
        "It achieved the lowest RMSE of 3.08 and a near-perfect R² Score of 0.9995, outperforming all other models in accuracy and consistency, making it the most effective for stock price prediction and reliable business decision-making.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Linear Regression as the final model since it showed the highest prediction accuracy with the lowest error.\n",
        "It models the relationship between the current stock closing price and the previous day's closing price (Prev_Close) using a simple linear equation.\n",
        "Due to the strong correlation between these two features in the dataset, the model was able to fit the trend exceptionally well.\n",
        "To understand feature importance, we examined the coefficient of the Prev_Close variable, which directly indicates its influence on the predicted value.\n",
        "Since this was a single-feature model, Prev_Close held 100% of the predictive power, and no advanced explainability tools were required."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(model, 'linear_regression_model.joblib')"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the model from file\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n",
        "\n",
        "# Wrap your unseen input with column name\n",
        "unseen_df = pd.DataFrame([[95.20]], columns=['Prev_Close'])\n",
        "prediction = loaded_model.predict(unseen_df)\n",
        "\n",
        "print(f\"📈 Predicted Close Price: ₹{prediction[0]:.2f}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('linear_regression_model.joblib')\n"
      ],
      "metadata": {
        "id": "mdbIktgNYpXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we built a predictive machine learning system to forecast the closing stock prices of Yes Bank using historical data. We began with thorough data cleaning and feature engineering, including the creation of lag variables like Prev_Close, and explored the dataset through visualizations and statistical insights. Multiple machine learning models were implemented and evaluated — including Linear Regression, Random Forest, and Gradient Boosting Regressor — using RMSE and R² Score as the key evaluation metrics.\n",
        "\n",
        "Among all models, Linear Regression without tuning outperformed others with an RMSE of 3.08 and an R² of 0.9995, making it the most suitable model for deployment. We further enhanced other models using GridSearchCV to tune hyperparameters, and validated performance improvements through metric comparisons. Finally, we saved the best model in .joblib format, successfully reloaded it, and performed a sanity prediction on unseen data — confirming it's deployment-ready.\n",
        "\n",
        "This project demonstrates a complete ML pipeline — from data preprocessing and EDA to model building, optimization, evaluation, and saving for production — making it a reliable foundation for future enhancements or business integration.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}
